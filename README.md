# TBP Data Pipeline

> **Sistema de ingesta de datos financieros desde Polygon.io S3 a TimescaleDB**

![Status](https://img.shields.io/badge/Status-Implementation-orange)
![Progress](https://img.shields.io/badge/Progress-65%25-green)
![Python](https://img.shields.io/badge/Python-3.11-blue)
![Database](https://img.shields.io/badge/Database-TimescaleDB-purple)

Sistema de pipeline de datos financieros para TBP (Trading Bot Project). Descarga autom√°tica de datos hist√≥ricos de mercado desde Polygon.io S3 y almacenamiento en TimescaleDB con validaci√≥n de calidad y retry logic robusto.

## üéØ MVP Scope

- **12 tickers**: 10 stocks + 2 ETFs del portfolio personal
- **Timeframe**: Solo datos diarios (1d)
- **Hist√≥rico**: 5 a√±os (2020-2024)
- **Fuente**: Polygon.io S3 Flat Files (primaria) + API (fallback)
- **Base de datos**: TimescaleDB con hypertables

## ‚úÖ Estado de Implementaci√≥n (65% completo)

### Componentes Completados
- **‚úÖ ConfigLoader**: Sistema configuraci√≥n YAML con template substitution
- **‚úÖ S3Client**: Cliente descarga completo con retry logic y validaci√≥n
- **‚úÖ Database Schema**: TimescaleDB schema con hypertables e √≠ndices
- **‚úÖ Testing**: Framework testing con LocalStack y pytest

### En Progreso
- **üîÑ Database Client**: Cliente modular TimescaleDB con connection pooling
- **üìã Main Orchestration**: L√≥gica principal coordinaci√≥n pipeline

## üöÄ Quick Start

### Prerequisites
- Python 3.11+
- PostgreSQL 15+ con TimescaleDB 2.15+
- Polygon.io S3 credentials

### Instalaci√≥n
```bash
# Clonar repositorio
git clone https://github.com/patofp/tbp-data-pipeline
cd tbp-data-pipeline

# Crear virtual environment
python -m venv .venv
source .venv/bin/activate  # Linux/Mac
# .venv\\Scripts\\activate  # Windows

# Instalar dependencias
pip install -r requirements.txt

# Configurar environment
cp .env.example .env
# Editar .env con tus credenciales

# Cargar variables de entorno
source scripts/set-env-vars.sh

# Ejecutar migraciones database
python scripts/run_migrations.py upgrade
```

### Variables de Entorno Requeridas
```bash
# S3 credentials
POLYGON_S3_ACCESS_KEY=your_access_key
POLYGON_S3_SECRET_KEY=your_secret_key

# Database credentials  
DB_HOST=192.168.1.11
DB_PORT=5432
DB_NAME=trading
DB_USER=your_user
DB_PASSWORD=your_password

# Opcional
POLYGON_API_KEY=your_api_key
LOG_LEVEL=INFO
ENVIRONMENT=development
```

## üèóÔ∏è Arquitectura

### Flujo de Datos
```
Polygon.io S3 ‚Üí S3Client ‚Üí DataValidation ‚Üí TimescaleDB
                   ‚Üì
              FailedDownload Tracking ‚Üí Retry Logic
```

### Componentes Principales
```
src/
‚îú‚îÄ‚îÄ config_loader.py       # ‚úÖ Gesti√≥n configuraci√≥n YAML
‚îú‚îÄ‚îÄ s3_client.py           # ‚úÖ Cliente Polygon.io S3
‚îî‚îÄ‚îÄ database/              # üîÑ Cliente modular TimescaleDB
    ‚îú‚îÄ‚îÄ client.py          #     Coordinador principal
    ‚îú‚îÄ‚îÄ market_data.py     #     Operaciones market_data_raw
    ‚îú‚îÄ‚îÄ failed_download.py #     Tracking fallos
    ‚îî‚îÄ‚îÄ data_quality.py    #     M√©tricas calidad
```

### Schema Database
```sql
-- Tabla principal datos mercado (hypertable)
market_data_raw (ticker, timestamp, timeframe, data_source, OHLC, volume, transactions)

-- Tracking fallos para retry
failed_downloads (ticker, date, error_type, attempts, resolved_at)

-- M√©tricas calidad datos
data_quality_metrics (ticker, date, quality_score, rejection_reasons)
```

## üìä Configuraci√≥n

### 12 Tickers Configurados
```yaml
# High Priority (Mega cap tech)
["AAPL", "MSFT", "NVDA", "META"]

# Medium Priority (Large cap diversified)  
["JPM", "V", "CVX", "LLY", "UNH", "WMT"]

# ETFs (Market analysis)
["SPY", "QQQ"]
```

### Template Substitution
Todos los secrets usan template substitution:
```yaml
s3_config:
  credentials:
    access_key: "${POLYGON_S3_ACCESS_KEY}"
    secret_key: "${POLYGON_S3_SECRET_KEY}"
    
database:
  connection:
    host: "${DB_HOST}"
    password: "${DB_PASSWORD}"
```

## üß™ Testing

```bash
# Tests unitarios (r√°pidos, con mocks)
pytest -m unit

# Tests integraci√≥n (requiere Docker para LocalStack)
pytest -m integration

# Suite completa con coverage
pytest --cov=src --cov-report=term-missing

# Test espec√≠fico
pytest test/test_config_loader.py -v
```

### LocalStack S3 Testing
Los tests de S3Client usan LocalStack para testing aislado:
```python
@pytest.fixture
def localstack_s3_client():
    # Configuraci√≥n LocalStack con datos Polygon.io reales
    yield s3_client
```

## üîß Uso

### Test Configuraci√≥n
```bash
# Verificar carga configuraci√≥n
python -c "from src.config_loader import ConfigLoader; cl = ConfigLoader(); print(f'Loaded {len(cl.get_all_tickers())} tickers')"

# Verificar conexi√≥n S3
python -c "from src.s3_client import PolygonS3Client; from src.config_loader import ConfigLoader; cl = ConfigLoader(); client = PolygonS3Client(cl.get_s3_config()); print('S3 Connected!')"

# Verificar database (cuando est√© implementado)
python -c "from src.database import TimescaleDBClient; client = TimescaleDBClient(); print(f'DB Connected: {client.test_connection()}')"
```

### Download Ejemplo (implementaci√≥n futura)
```bash
# Download single ticker
python scripts/download_historical.py --ticker AAPL --start 2024-01-01 --end 2024-12-31

# Download all tickers
python scripts/download_historical.py --all --start 2024-01-01 --end 2024-12-31

# Dry run
python scripts/download_historical.py --ticker AAPL --dry-run
```

## üìà Performance

### M√©tricas Actuales
- **ConfigLoader**: <50ms carga configuraci√≥n
- **S3Client**: ~100 rows/segundo parsing CSV
- **Memory**: ~150KB por batch diario
- **Database**: Target >1K rows/segundo (en implementaci√≥n)

### Targets MVP
- **Data Completeness**: >95% coverage
- **Daily Processing**: <15 minutos para 12 tickers
- **Storage**: ~15K records (5 a√±os √ó 12 tickers √ó 252 d√≠as trading)
- **Memory Usage**: <1GB durante procesamiento

## üõ°Ô∏è Calidad de Datos

### Validaci√≥n OHLC
```yaml
validation_rules:
  - high >= low
  - high >= open, close  
  - low <= open, close
  - volume >= 0
  - prices > 0
```

### Estrategia NaN
- **OHLC**: Rechazo registro completo
- **Volume**: Convertir a 0
- **Transactions**: Mantener NULL

### Error Handling
- Retry autom√°tico con exponential backoff
- Rate limiting detection
- FailedDownload tracking para Airflow
- Categorizaci√≥n errores (network, parsing, quality)

## üîó Documentaci√≥n

### Arquitectura y Decisiones
- **[System Overview](docs/architecture/system-overview.md)**: Arquitectura completa actual
- **[Data Pipeline Decisions](docs/architecture/data-pipeline-decisions.md)**: Decisiones t√©cnicas tomadas
- **[Progress Tracker](tbp-data-pipeline-tracker.md)**: Estado desarrollo actual

### Setup y Troubleshooting
- **[Database Setup](docs/database-setup.md)**: Configuraci√≥n TimescaleDB
- **[CLAUDE.md](CLAUDE.md)**: Context para LLMs
- **[PLAN.md](PLAN.md)**: Plan desarrollo detallado

## üìã Roadmap

### Inmediato (Esta Semana)
- [ ] Completar Database Client modular
- [ ] Implementar MarketDataClient con COPY protocol
- [ ] Testing integraci√≥n database

### Corto Plazo (Pr√≥xima Semana)
- [ ] Main orchestration logic (downloader.py)
- [ ] CLI execution script (download_historical.py)
- [ ] Data quality validation framework
- [ ] End-to-end integration testing

### Mediano Plazo (Siguientes Semanas)
- [ ] Airflow DAGs para execution diaria
- [ ] Monitoring y alerting b√°sico
- [ ] CI/CD pipeline setup
- [ ] Performance optimization

### Largo Plazo
- [ ] Integraci√≥n con tbp-feature-lab
- [ ] Support para minute-level data
- [ ] Real-time data streaming
- [ ] Multi-asset class support

## ü§ù Contributing

### Development Guidelines
1. **Configuration-driven**: Toda configuraci√≥n en YAML
2. **Test-first**: Escribir tests antes que c√≥digo
3. **Error handling**: Categorizar errores, implementar retry logic
4. **Logging**: Structured JSON logs con context
5. **Performance**: Target >1K rows/sec inserts, <1GB memory

### Code Standards
- Python 3.11 con type hints en todas partes
- Seguir patrones existentes (mirar s3_client.py como referencia)
- Documentar decisiones y l√≥gica compleja
- Manejar weekends/holidays autom√°ticamente
- Usar connection pool, nunca crear conexiones directas

### Testing Strategy
```bash
# Unit tests (fast, mocked) - EJECUTAR FRECUENTEMENTE
pytest -m unit

# Integration tests (Docker required) - EJECUTAR ANTES DE COMMITS
pytest -m integration

# Full test suite with coverage
pytest --cov=src --cov-report=term-missing
```

## ‚ö†Ô∏è Important Rules

1. **NUNCA** commit sin ejecutar tests
2. **NUNCA** hardcodear credentials o secrets
3. **NUNCA** crear database connections fuera del connection pool
4. **SIEMPRE** manejar missing data gracefully (weekends, holidays)
5. **SIEMPRE** validar datos antes de insertar en database

## üìä Success Metrics

- **Data Completeness**: >95% coverage
- **Daily Processing**: <15 minutos para todos los tickers
- **Test Coverage**: >80%
- **Insert Performance**: >1K rows/segundo
- **Error Recovery**: Retry autom√°tico exitoso >90%

## üÜò Troubleshooting

### Database Issues
```bash
# Conectar a test DB
psql -h localhost -p 5433 -U test_user -d test_db

# Ver logs
docker logs tbp-data-pipeline-postgres-test-1

# Verificar hypertables
SELECT * FROM timescaledb_information.hypertables;
```

### S3 Connection Issues
```bash
# Test S3 connectivity
aws s3 ls s3://flatfiles/us_stocks_sip/day_aggs_v1/2024/01/ --endpoint-url=https://files.polygon.io

# Verificar credentials
echo $POLYGON_S3_ACCESS_KEY
echo $POLYGON_S3_SECRET_KEY
```

### Configuration Issues
```bash
# Verificar template substitution
python -c "from src.config_loader import ConfigLoader; cl = ConfigLoader(); print(cl.get_s3_config())"

# Listar variables entorno
env | grep -E '(POLYGON|DB_)'
```

## üìù License

Proprietary - Trading Bot Project

## üîó Related Projects

- **tbp-feature-lab**: Feature engineering module (siguiente)
- **tbp-ml-engine**: ML training y deployment
- **tbp-bot-strategies**: Trading strategies implementation
- **tbp-paper-trading**: Paper trading orchestrator

---

**Recuerda**: Este es financial data - precisi√≥n y confiabilidad son primordiales!

Para m√°s detalles, ver documentaci√≥n completa en `/docs/` o contactar al team.

**Repository**: https://github.com/patofp/tbp-data-pipeline  
**Documentation**: Obsidian vault en `/mnt/d/obsidian-vaults/patofp/tbp-docs/`  
**Status**: Implementation Phase (65% complete)  
**Last Updated**: January 2025
